import random
import os
import numpy as np
import torch
import csv
import pandas as pd
import cv2
from sklearn.model_selection import GroupKFold
from torch.utils.data import Dataset
import albumentations as A
from albumentations.pytorch.transforms import ToTensorV2
from torch import nn
from torchvision import models
import torch.nn.functional as F
from sklearn.metrics import precision_score, recall_score, f1_score
from datetime import datetime
import time
from catalyst.data.sampler import BalanceClassSampler
from torch.utils.data.sampler import SequentialSampler

SEED = 512
NUM_CLASSES = 9

def seed_everything(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True

seed_everything(SEED)

DATASET_BASE_PATH = 'E:\\datasets\\malware-classification'
#DATASET_BASE_PATH = '/mnt/e/datasets/malware-classification'
LABEL_PATH = DATASET_BASE_PATH + '/trainLabels.csv'
DATA_PATH = DATASET_BASE_PATH + '/train'

dataset = []

with open(LABEL_PATH, 'r') as label_csv:
    label_reader = csv.reader(label_csv)
    next(label_reader)
    for row in label_reader:
        dataset.append({
            'label': int(row[1]) - 1, 
            'data_name': DATA_PATH + f'/{row[0]}'
        })

random.shuffle(dataset)
dataset = pd.DataFrame(dataset)
gkf = GroupKFold(n_splits=13)
dataset.loc[:, 'fold'] = 0

for fold_number, (train_index, val_index) in enumerate(gkf.split(X=dataset.index, y=dataset['label'], groups=dataset['data_name'])):
    dataset.loc[dataset.iloc[val_index].index, 'fold'] = fold_number

def onehot(size, target):
    vec = torch.zeros(size, dtype=torch.float32)
    vec[target] = 1.
    return vec

def to_grey_scale_image(input):
    len = input.shape[0]
    width = np.sqrt(len)
    width = np.ceil(width).astype(np.int64)
    padding = np.zeros((width ** 2 - len, ), dtype = np.int8)
    output = np.concatenate((input, padding))
    output = np.reshape(output, (width, width))
    return output

to_standard_size = A.Compose([
            A.HorizontalFlip(p=0.5),
            A.VerticalFlip(p=0.5),
            A.Resize(height=512, width=512, p=1.0),
            ToTensorV2(p=1.0),
        ], p=1.0)


class DatasetRetriever(Dataset):
    
    def __init__(self, labels, data_names, transforms, normalize):
        super().__init__()
        self.labels = labels
        self.data_names = data_names
        self.transforms = transforms
        self.normalize = normalize

    def __getitem__(self, index):
        label, data_name = self.labels[index], self.data_names[index]
        data_bytes_path = data_name + '.bytes'
        data_asm_path = data_name + '.asm'
        data_bytes_len = os.path.getsize(data_bytes_path)
        data_bytes = np.fromfile(data_bytes_path, dtype=np.uint8)
        image = self.transforms(data_bytes)
        image = self.normalize(image=image)["image"]
        target = onehot(NUM_CLASSES, label)
        return image, target

    def __len__(self):
        return self.data_names.shape[0]

    def get_labels(self):
        return list(self.labels)

fold_number = 0

train_dataset = DatasetRetriever(
    labels=dataset[dataset['fold'] != fold_number].label.values,
    data_names=dataset[dataset['fold'] != fold_number].data_name.values,
    transforms=to_grey_scale_image,
    normalize=to_standard_size
)

validation_dataset = DatasetRetriever(
    labels=dataset[dataset['fold'] == fold_number].label.values,
    data_names=dataset[dataset['fold'] == fold_number].data_name.values,
    transforms=to_grey_scale_image,
    normalize=to_standard_size
)

class visualization():

    def __init__(self, path) -> None:
        self.PATH = path

    def show_grey_scal_image(input):
        cv2.imshow('grey_scale', input)
        cv2.waitKey(0)
    
    def save_image(self, name, input):
        cv2.imwrite(f'{self.PATH}/{name}.jpg', input, [int(cv2.IMWRITE_JPEG_QUALITY), 100])

class BaselineModel(nn.Module):

    def __init__(self) -> None:
        super(BaselineModel, self).__init__()
        self.num_classes = 9
        vgg19 = models.vgg19(pretrained=True)
        vgg19.classifier = nn.Sequential(
            nn.Linear(512 * 7 * 7, 4096),
            nn.ReLU(True),
            nn.Dropout(p=0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(True),
            nn.Dropout(p=0.5),
            nn.Linear(4096, self.num_classes),
        )
        vgg19.features[0] = nn.Conv2d(
            1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        )
        self.model = vgg19

    def forward(self, x):
        x = self.model(x)
        return x

class AverageMeter(object):
    """Computes and stores the average and current value"""
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

class ScoreMeter(object):
    
    def __init__(self):
        self.reset()

    def reset(self):
        self.y_true = np.array([])
        self.y_pred = np.array([])
        self.precision = 0
        self.recall = 0
        self.f1_score = 0

    def update(self, y_true, y_pred):
        y_true = y_true.cpu().numpy().argmax(axis=1).astype(int)
        y_pred = nn.functional.softmax(y_pred, dim=1).data.cpu().numpy().argmax(axis=1).astype(int)
        self.y_true = np.hstack((self.y_true, y_true))
        self.y_pred = np.hstack((self.y_pred, y_pred))
        self.precision = precision_score(self.y_true, self.y_pred, average='micro')
        self.recall = recall_score(self.y_true, self.y_pred, average='micro')
        self.f1_score = f1_score(self.y_true, self.y_pred, average='micro')

    @property
    def avg(self):
        return self.precision, self.recall, self.f1_score

class Fitter:

    def __init__(self, model, device, config) -> None:
        self.config = config
        self.epoch = 0
        self.base_dir = '..'
        self.log_path = f'{self.base_dir}/logs/log.txt'
        self.best_summary_loss = 10**5
        self.model = model
        self.device = device
        self.optimizer = torch.optim.AdamW(self.model.parameters(),lr = self.config.lr)
        self.scheduler = self.config.SchedulerClass(self.optimizer, **self.config.scheduler_params)
        self.criterion = F.cross_entropy()
        self.log(f'Fitter prepared. Device is {self.device}')

        if self.config.from_last_checkpoints:
            self.load(f'{self.base_dir}/checkpoints/last-checkpoint.bin')

        if self.config.from_best_checkpoints:
            self.load(f'{self.base_dir}/checkpoints/best-checkpoint.bin')

    def fit(self, train_loader, validation_loader):
        for e in range(self.config.n_epochs):
            if self.config.verbose:
                lr = self.optimizer.param_groups[0]['lr']
                timestamp = datetime.utcnow().isoformat()
                self.log(f'\n{timestamp}\nLR: {lr}')
            
            t = time.time()
            summary_loss, final_scores = self.train_one_epoch(train_loader)
            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, \
                summary_loss: {summary_loss.avg:.5f}, \
                precision: {final_scores.avg[0]:.5f}, \
                recall: {final_scores.avg[1]:.5f}, \
                f1: {final_scores.avg[2]:.5f}, \
                time: {(time.time() - t):.5f}')
            self.save(f'{self.base_dir}/checkpoints/last-checkpoint.bin')

            t = time.time()
            summary_loss, final_scores = self.validation(validation_loader)
            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, \
                summary_loss: {summary_loss.avg:.5f}, \
                precision: {final_scores.avg[0]:.5f}, \
                recall: {final_scores.avg[1]:.5f}, \
                f1: {final_scores.avg[2]:.5f}, \
                time: {(time.time() - t):.5f}')

            if summary_loss.avg < self.best_summary_loss:
                self.best_summary_loss = summary_loss.avg
                self.model.eval()
                self.save(f'{self.base_dir}/checkpoints/best-checkpoint.bin')

            self.epoch += 1

    def train_one_epoch(self, train_loader):
        self.model.train()
        summary_loss = AverageMeter()
        final_scores = ScoreMeter()
        t = time.time()
        for step, (images, targets) in enumerate(train_loader):
            if self.config.verbose:
                if step % self.config.verbose_step == 0:
                    print(
                        f'Train Step {step}/{len(train_loader)}, ' + \
                        f'summary_loss: {summary_loss.avg:.5f}, ' + \
                        f'precision: {final_scores.avg[0]:.5f}, ' + \
                        f'recall: {final_scores.avg[1]:.5f}, ' + \
                        f'f1: {final_scores.avg[2]:.5f}, ' + \
                        f'time: {(time.time() - t):.5f}', end='\r'
                    )
            
            targets = targets.to(self.device).float()
            images = images.to(self.device).float()
            batch_size = images.shape[0]
            self.optimizer.zero_grad()
            outputs = self.model(images)
            loss = self.criterion(outputs, targets)
            loss.backward()
            final_scores.update(targets, outputs)
            summary_loss.update(loss.detach().item(), batch_size)
            self.optimizer.step()
            self.scheduler.step(summary_loss.avg)

        return summary_loss, final_scores

    def validation(self, validation_loader):
        self.model.eval()
        summary_loss = AverageMeter()
        final_scores = ScoreMeter()
        t = time.time()
        for step, (images, targets) in enumerate(validation_loader):
            if self.config.verbose:
                if step % self.config.verbose_step == 0:
                    print(
                        f'Train Step {step}/{len(validation_loader)}, ' + \
                        f'summary_loss: {summary_loss.avg:.5f}, ' + \
                        f'precision: {final_scores.avg[0]:.5f}, ' + \
                        f'recall: {final_scores.avg[1]:.5f}, ' + \
                        f'f1: {final_scores.avg[2]:.5f}, ' + \
                        f'time: {(time.time() - t):.5f}', end='\r'
                    )
            
            with torch.no_grad():
                targets = targets.to(self.device).float()
                batch_size = images.shape[0]
                images = images.to(self.device).float()
                outputs = self.model(images)
                loss = self.criterion(outputs, targets)
                final_scores.update(targets, outputs)
                summary_loss.update(loss.detach().item(), batch_size)

        return summary_loss, final_scores

    def save(self, path):
        self.model.eval()
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_summary_loss': self.best_summary_loss,
            'epoch': self.epoch,
        }, path)

    def load(self, path):
        checkpoint = torch.load(path)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.best_summary_loss = checkpoint['best_summary_loss']
        self.epoch = checkpoint['epoch'] + 1
        
    def log(self, message):
        if self.config.verbose:
            print(message)
        with open(self.log_path, 'a+') as logger:
            logger.write(f'{message}\n')

class TrainGlobalConfig:
    num_workers = 4
    batch_size = 16
    n_epochs = 10
    lr = 0.001

    verbose = True
    verbose_step = 1

    from_last_checkpoints = False
    from_best_checkpoints = False

#     SchedulerClass = torch.optim.lr_scheduler.OneCycleLR
#     scheduler_params = dict(
#         max_lr=0.001,
#         epochs=n_epochs,
#         steps_per_epoch=int(len(train_dataset) / batch_size),
#         pct_start=0.1,
#         anneal_strategy='cos', 
#         final_div_factor=10**5
#     )
    
    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau
    scheduler_params = dict(
        mode='min',
        factor=0.6,
        patience=10,
        verbose=False, 
        threshold=0.001,
        threshold_mode='abs',
        cooldown=0, 
        min_lr=1e-9,
        eps=1e-8
    )

net = BaselineModel().cuda()

def run_training():
    device = torch.device('cuda:0')
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        sampler=BalanceClassSampler(labels=train_dataset.get_labels(), mode="downsampling"),
        batch_size=TrainGlobalConfig.batch_size,
        pin_memory=False,
        drop_last=True,
        num_workers=TrainGlobalConfig.num_workers,
    )
    val_loader = torch.utils.data.DataLoader(
        validation_dataset, 
        batch_size=TrainGlobalConfig.batch_size,
        num_workers=TrainGlobalConfig.num_workers,
        shuffle=False,
        sampler=SequentialSampler(validation_dataset),
        pin_memory=False,
    )
    fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)
    fitter.fit(train_loader, val_loader)

run_training()